{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7b0b4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "342040e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d451648a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patients Loaded: 1251\n"
     ]
    }
   ],
   "source": [
    "images_path = 'Data/BraTS2021'\n",
    "IMG_SIZE = 128\n",
    "VOLUME_SLICES = 50\n",
    "VOLUME_START_AT = 22\n",
    "\n",
    "# Load dataset\n",
    "all_patients = [os.path.join(images_path, p) for p in os.listdir(images_path)]\n",
    "np.random.shuffle(all_patients)\n",
    "\n",
    "print(f\"Total Patients Loaded: {len(all_patients)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13b06046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patients per Hospital: [417, 417, 417]\n"
     ]
    }
   ],
   "source": [
    "nodes = {\n",
    "    \"Hospital_1\": all_patients[:len(all_patients)//3],\n",
    "    \"Hospital_2\": all_patients[len(all_patients)//3:2*len(all_patients)//3],\n",
    "    \"Hospital_3\": all_patients[2*len(all_patients)//3:]\n",
    "}\n",
    "\n",
    "print(f\"Total Patients per Hospital: {[len(nodes[h]) for h in nodes.keys()]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6af3464",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BraTSDataset(Dataset):\n",
    "    def __init__(self, file_list):\n",
    "        self.file_list = file_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        case_path = os.path.join(images_path, os.path.basename(self.file_list[idx]), os.path.basename(self.file_list[idx]))\n",
    "\n",
    "        flair = nib.load(f'{case_path}_flair.nii').get_fdata()\n",
    "        ce = nib.load(f'{case_path}_t1ce.nii').get_fdata()\n",
    "        seg = nib.load(f'{case_path}_seg.nii').get_fdata()\n",
    "\n",
    "        flair = cv2.resize(flair[:, :, 22], (128, 128))\n",
    "        ce = cv2.resize(ce[:, :, 22], (128, 128))\n",
    "        seg = cv2.resize(seg[:, :, 22], (128, 128))\n",
    "\n",
    "        # Ensure all labels are within the expected range [0,1,2,3]\n",
    "        seg[seg == 4] = 3  # Map class 4 to 3\n",
    "\n",
    "        image = np.stack([flair, ce], axis=0)\n",
    "        label = seg.astype(np.int64)\n",
    "\n",
    "        return torch.tensor(image, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f2a71091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hospital_1 - Train: 291, Val: 62, Test: 64\n",
      "Hospital_2 - Train: 291, Val: 62, Test: 64\n",
      "Hospital_3 - Train: 291, Val: 62, Test: 64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "hospital_loaders = {}\n",
    "\n",
    "train_ratio, val_ratio, test_ratio = 0.7, 0.15, 0.15\n",
    "\n",
    "for hospital, files in nodes.items():\n",
    "    dataset = BraTSDataset(files)\n",
    "    \n",
    "    # Compute dataset sizes\n",
    "    train_size = int(train_ratio * len(dataset))\n",
    "    val_size = int(val_ratio * len(dataset))\n",
    "    test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "    # Perform split\n",
    "    train_set, val_set, test_set = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "    # Create dataloaders\n",
    "    hospital_loaders[hospital] = {\n",
    "        \"train\": DataLoader(train_set, batch_size=batch_size, shuffle=True),\n",
    "        \"val\": DataLoader(val_set, batch_size=batch_size, shuffle=False),\n",
    "        \"test\": DataLoader(test_set, batch_size=batch_size, shuffle=False),\n",
    "    }\n",
    "\n",
    "    print(f\"{hospital} - Train: {train_size}, Val: {val_size}, Test: {test_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d55a0af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=2, out_channels=4):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        def conv_block(in_ch, out_ch):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "\n",
    "        self.encoder1 = conv_block(in_channels, 64)\n",
    "        self.encoder2 = conv_block(64, 128)\n",
    "        self.encoder3 = conv_block(128, 256)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "        self.bottleneck = conv_block(256, 512)\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.decoder3 = conv_block(512, 256)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.decoder2 = conv_block(256, 128)\n",
    "\n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.decoder1 = conv_block(128, 64)\n",
    "\n",
    "        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.encoder1(x)\n",
    "        e2 = self.encoder2(self.pool(e1))\n",
    "        e3 = self.encoder3(self.pool(e2))\n",
    "\n",
    "        b = self.bottleneck(self.pool(e3))\n",
    "\n",
    "        d3 = self.decoder3(torch.cat([self.upconv3(b), e3], dim=1))\n",
    "        d2 = self.decoder2(torch.cat([self.upconv2(d3), e2], dim=1))\n",
    "        d1 = self.decoder1(torch.cat([self.upconv1(d2), e1], dim=1))\n",
    "\n",
    "        return self.final_conv(d1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a3a5f87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def federated_averaging(models):\n",
    "    new_model = UNet().to(device)\n",
    "    new_state_dict = new_model.state_dict()\n",
    "\n",
    "    for key in new_state_dict.keys():\n",
    "        new_state_dict[key] = torch.stack([models[i][key] for i in range(len(models))], dim=0).mean(dim=0)\n",
    "\n",
    "    new_model.load_state_dict(new_state_dict)\n",
    "    return new_model.state_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f8193c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Hospital_1 - Round 1\n",
      "Training Hospital_2 - Round 1\n",
      "Training Hospital_3 - Round 1\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[WinError 8] Not enough memory resources are available to process this command",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m     17\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m dataloader[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m     19\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     21\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\binwa\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    714\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\binwa\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\binwa\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\binwa\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[1;32mIn[26], line 11\u001b[0m, in \u001b[0;36mBraTSDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m      9\u001b[0m     case_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(images_path, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_list[idx]), os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_list[idx]))\n\u001b[1;32m---> 11\u001b[0m     flair \u001b[38;5;241m=\u001b[39m nib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcase_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_flair.nii\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mget_fdata()\n\u001b[0;32m     12\u001b[0m     ce \u001b[38;5;241m=\u001b[39m nib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcase_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_t1ce.nii\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mget_fdata()\n\u001b[0;32m     13\u001b[0m     seg \u001b[38;5;241m=\u001b[39m nib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcase_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_seg.nii\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mget_fdata()\n",
      "File \u001b[1;32mc:\\Users\\binwa\\anaconda3\\Lib\\site-packages\\nibabel\\dataobj_images.py:373\u001b[0m, in \u001b[0;36mDataobjImage.get_fdata\u001b[1;34m(self, caching, dtype)\u001b[0m\n\u001b[0;32m    369\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fdata_cache\n\u001b[0;32m    370\u001b[0m \u001b[38;5;66;03m# Always return requested data type\u001b[39;00m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;66;03m# For array proxies, will attempt to confine data array to dtype\u001b[39;00m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;66;03m# during scaling\u001b[39;00m\n\u001b[1;32m--> 373\u001b[0m data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataobj, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m caching \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfill\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    375\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fdata_cache \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[1;32mc:\\Users\\binwa\\anaconda3\\Lib\\site-packages\\nibabel\\arrayproxy.py:457\u001b[0m, in \u001b[0;36mArrayProxy.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    437\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Read data from file and apply scaling, casting to ``dtype``\u001b[39;00m\n\u001b[0;32m    438\u001b[0m \n\u001b[0;32m    439\u001b[0m \u001b[38;5;124;03m    If ``dtype`` is unspecified, the dtype of the returned array is the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;124;03m        Scaled image data with type `dtype`.\u001b[39;00m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 457\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_scaled(dtype\u001b[38;5;241m=\u001b[39mdtype, slicer\u001b[38;5;241m=\u001b[39m())\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    459\u001b[0m         arr \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\binwa\\anaconda3\\Lib\\site-packages\\nibabel\\arrayproxy.py:424\u001b[0m, in \u001b[0;36mArrayProxy._get_scaled\u001b[1;34m(self, dtype, slicer)\u001b[0m\n\u001b[0;32m    422\u001b[0m     scl_inter \u001b[38;5;241m=\u001b[39m scl_inter\u001b[38;5;241m.\u001b[39mastype(use_dtype)\n\u001b[0;32m    423\u001b[0m \u001b[38;5;66;03m# Read array and upcast as necessary for big slopes, intercepts\u001b[39;00m\n\u001b[1;32m--> 424\u001b[0m scaled \u001b[38;5;241m=\u001b[39m apply_read_scaling(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_unscaled(slicer\u001b[38;5;241m=\u001b[39mslicer), scl_slope, scl_inter)\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    426\u001b[0m     scaled \u001b[38;5;241m=\u001b[39m scaled\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mpromote_types(scaled\u001b[38;5;241m.\u001b[39mdtype, dtype), copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\binwa\\anaconda3\\Lib\\site-packages\\nibabel\\arrayproxy.py:394\u001b[0m, in \u001b[0;36mArrayProxy._get_unscaled\u001b[1;34m(self, slicer)\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m canonical_slicers(slicer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m==\u001b[39m canonical_slicers(\n\u001b[0;32m    391\u001b[0m     (), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    392\u001b[0m ):\n\u001b[0;32m    393\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_fileobj() \u001b[38;5;28;01mas\u001b[39;00m fileobj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m--> 394\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m array_from_file(\n\u001b[0;32m    395\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape,\n\u001b[0;32m    396\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dtype,\n\u001b[0;32m    397\u001b[0m             fileobj,\n\u001b[0;32m    398\u001b[0m             offset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_offset,\n\u001b[0;32m    399\u001b[0m             order\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morder,\n\u001b[0;32m    400\u001b[0m             mmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mmap,\n\u001b[0;32m    401\u001b[0m         )\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_fileobj() \u001b[38;5;28;01mas\u001b[39;00m fileobj:\n\u001b[0;32m    403\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fileslice(\n\u001b[0;32m    404\u001b[0m         fileobj,\n\u001b[0;32m    405\u001b[0m         slicer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    410\u001b[0m         lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock,\n\u001b[0;32m    411\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\binwa\\anaconda3\\Lib\\site-packages\\nibabel\\volumeutils.py:450\u001b[0m, in \u001b[0;36marray_from_file\u001b[1;34m(shape, in_dtype, infile, offset, order, mmap)\u001b[0m\n\u001b[0;32m    448\u001b[0m mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mmap \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m mmap\n\u001b[0;32m    449\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:  \u001b[38;5;66;03m# Try memmapping file on disk\u001b[39;00m\n\u001b[1;32m--> 450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmemmap(infile, in_dtype, mode\u001b[38;5;241m=\u001b[39mmode, shape\u001b[38;5;241m=\u001b[39mshape, order\u001b[38;5;241m=\u001b[39morder, offset\u001b[38;5;241m=\u001b[39moffset)\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;66;03m# The error raised by memmap, for different file types, has\u001b[39;00m\n\u001b[0;32m    452\u001b[0m     \u001b[38;5;66;03m# changed in different incarnations of the numpy routine\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\binwa\\anaconda3\\Lib\\site-packages\\numpy\\core\\memmap.py:268\u001b[0m, in \u001b[0;36mmemmap.__new__\u001b[1;34m(subtype, filename, dtype, mode, offset, shape, order)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28mbytes\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m start\n\u001b[0;32m    267\u001b[0m array_offset \u001b[38;5;241m=\u001b[39m offset \u001b[38;5;241m-\u001b[39m start\n\u001b[1;32m--> 268\u001b[0m mm \u001b[38;5;241m=\u001b[39m mmap\u001b[38;5;241m.\u001b[39mmmap(fid\u001b[38;5;241m.\u001b[39mfileno(), \u001b[38;5;28mbytes\u001b[39m, access\u001b[38;5;241m=\u001b[39macc, offset\u001b[38;5;241m=\u001b[39mstart)\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m ndarray\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(subtype, shape, dtype\u001b[38;5;241m=\u001b[39mdescr, buffer\u001b[38;5;241m=\u001b[39mmm,\n\u001b[0;32m    271\u001b[0m                        offset\u001b[38;5;241m=\u001b[39marray_offset, order\u001b[38;5;241m=\u001b[39morder)\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mmap \u001b[38;5;241m=\u001b[39m mm\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 8] Not enough memory resources are available to process this command"
     ]
    }
   ],
   "source": [
    "global_model = UNet().to(device)\n",
    "num_rounds = 3\n",
    "learning_rate = 0.001\n",
    "\n",
    "for round in range(num_rounds):\n",
    "    local_weights = []\n",
    "\n",
    "    for hospital, dataloader in hospital_loaders.items():\n",
    "        print(f\"Training {hospital} - Round {round+1}\")\n",
    "\n",
    "        model = UNet().to(device)\n",
    "        model.load_state_dict(global_model.state_dict())\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        model.train()\n",
    "        for images, labels in dataloader[\"train\"]:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        local_weights.append(model.state_dict())\n",
    "\n",
    "    new_global_weights = federated_averaging(local_weights)\n",
    "    global_model.load_state_dict(new_global_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1a5c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coefficient(y_true, y_pred, smooth=1e-6):\n",
    "    \"\"\"Computes Dice Score for segmentation.\"\"\"\n",
    "    y_true_f = y_true.flatten()\n",
    "    y_pred_f = y_pred.flatten()\n",
    "    intersection = (y_true_f * y_pred_f).sum()\n",
    "    return (2. * intersection + smooth) / (y_true_f.sum() + y_pred_f.sum() + smooth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b2e855b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating models for Hospital_1...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dice_coefficient' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m y_pred_fl \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(fl_preds)\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Compute Dice Score\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m dice_fl \u001b[38;5;241m=\u001b[39m dice_coefficient(y_true, y_pred_fl)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Compute Confusion Matrix\u001b[39;00m\n\u001b[0;32m     32\u001b[0m cm_fl \u001b[38;5;241m=\u001b[39m confusion_matrix(y_true, y_pred_fl, labels\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dice_coefficient' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "import seaborn as sns\n",
    "\n",
    "# Define tumor class labels\n",
    "tumor_labels = [\"Background\", \"Edema\", \"Enhancing Tumor\", \"Tumor Core\"]\n",
    "\n",
    "# Evaluate FL model\n",
    "evaluation_results = {}\n",
    "\n",
    "for hospital, data_loader in hospital_loaders.items():\n",
    "    print(f\"Evaluating models for {hospital}...\")\n",
    "\n",
    "    fl_preds, y_trues = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader[\"test\"]:\n",
    "            images, labels = images.to(device), labels.cpu().numpy()\n",
    "\n",
    "            # FL Model Predictions\n",
    "            fl_outputs = global_model(images).cpu().numpy()\n",
    "            fl_preds.append(np.argmax(fl_outputs, axis=1))\n",
    "            y_trues.append(labels)\n",
    "\n",
    "    # Flatten predictions & ground truth\n",
    "    y_true = np.concatenate(y_trues).flatten()\n",
    "    y_pred_fl = np.concatenate(fl_preds).flatten()\n",
    "\n",
    "    # Compute Dice Score\n",
    "    dice_fl = dice_coefficient(y_true, y_pred_fl)\n",
    "\n",
    "    # Compute Confusion Matrix\n",
    "    cm_fl = confusion_matrix(y_true, y_pred_fl, labels=[0, 1, 2, 3])\n",
    "\n",
    "    # Compute ROC-AUC Score\n",
    "    fpr_fl, tpr_fl, _ = roc_curve(y_true.ravel(), y_pred_fl.ravel(), pos_label=1)\n",
    "    auc_fl = auc(fpr_fl, tpr_fl)\n",
    "\n",
    "    # Store results\n",
    "    evaluation_results[hospital] = {\n",
    "        \"Dice FL\": dice_fl,\n",
    "        \"AUC FL\": auc_fl,\n",
    "        \"Confusion Matrix FL\": cm_fl,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c5e56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract dice scores\n",
    "dice_scores = np.array([[evaluation_results[h][\"Dice FL\"]] for h in hospital_loaders.keys()])\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.heatmap(dice_scores, annot=True, cmap=\"coolwarm\", xticklabels=[\"FL Model\"], yticklabels=list(hospital_loaders.keys()))\n",
    "plt.title(\"Dice Score Heatmap\")\n",
    "plt.xlabel(\"Model Type\")\n",
    "plt.ylabel(\"Hospital\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4aad4232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for Hospital_1 - FL Model\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Hospital_1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hospital \u001b[38;5;129;01min\u001b[39;00m hospital_loaders\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfusion Matrix for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhospital\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - FL Model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m     plot_confusion_matrix(evaluation_results[hospital][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfusion Matrix FL\u001b[39m\u001b[38;5;124m\"\u001b[39m], title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhospital\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - FL Model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Hospital_1'"
     ]
    }
   ],
   "source": [
    "def plot_confusion_matrix(cm, title=\"Confusion Matrix\"):\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=tumor_labels, yticklabels=tumor_labels)\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"Actual Label\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "for hospital in hospital_loaders.keys():\n",
    "    print(f\"Confusion Matrix for {hospital} - FL Model\")\n",
    "    plot_confusion_matrix(evaluation_results[hospital][\"Confusion Matrix FL\"], title=f\"{hospital} - FL Model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fca1522",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "for hospital in hospital_loaders.keys():\n",
    "    fpr_fl, tpr_fl, _ = roc_curve(y_true.ravel(), y_pred_fl.ravel(), pos_label=1)\n",
    "    auc_fl = evaluation_results[hospital][\"AUC FL\"]\n",
    "\n",
    "    plt.plot(fpr_fl, tpr_fl, label=f\"{hospital} - FL Model (AUC = {auc_fl:.2f})\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], \"k--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curves for FL Model\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14359810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tumor_overlay(image, ground_truth, prediction, title=\"Tumor Overlay\"):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    axes[0].imshow(image[0, :, :], cmap=\"gray\")\n",
    "    axes[0].set_title(\"FLAIR MRI\")\n",
    "\n",
    "    axes[1].imshow(ground_truth, cmap=\"jet\", alpha=0.6)\n",
    "    axes[1].set_title(\"Ground Truth\")\n",
    "\n",
    "    axes[2].imshow(prediction, cmap=\"jet\", alpha=0.6)\n",
    "    axes[2].set_title(\"Predicted Segmentation\")\n",
    "\n",
    "    plt.suptitle(title)\n",
    "    plt.show()\n",
    "\n",
    "# Select a sample patient from Hospital 1\n",
    "sample_image, sample_label = next(iter(hospital_loaders[\"Hospital_1\"][\"test\"]))\n",
    "sample_image = sample_image[0].cpu().numpy()\n",
    "sample_label = sample_label[0].cpu().numpy()\n",
    "\n",
    "# Run FL Model\n",
    "with torch.no_grad():\n",
    "    fl_pred = global_model(sample_image.unsqueeze(0).to(device)).cpu().numpy().argmax(axis=1)[0]\n",
    "\n",
    "# Plot FL Prediction\n",
    "plot_tumor_overlay(sample_image, sample_label, fl_pred, title=\"FL Model Tumor Segmentation\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
