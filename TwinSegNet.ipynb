{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cd53492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# SECTION 1: SETUP & IMPORTS\n",
    "\n",
    "# Core ML\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import torchio as tio\n",
    "import warnings\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "\n",
    "# System and Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2c7ce96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations\n",
    "IMG_SIZE = 128\n",
    "VOLUME_SLICES = 50\n",
    "VOLUME_START_AT = 22\n",
    "BATCH_SIZE = 4\n",
    "NUM_EPOCHS = 5\n",
    "NUM_ROUNDS = 2\n",
    "batch_size = 4  # stable\n",
    "pin_memory = True\n",
    "num_workers = 4  # or os.cpu_count() // 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a66a7e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL: Fixed and Safe BraTSDataset\n",
    "\n",
    "class BraTSDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, train=True):\n",
    "        self.root_dir = root_dir\n",
    "        self.patient_dirs = sorted(os.listdir(root_dir))\n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patient_dirs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        patient_id = self.patient_dirs[idx]\n",
    "        patient_path = os.path.join(self.root_dir, patient_id)\n",
    "\n",
    "        # Load MRI modalities\n",
    "        modalities = ['t1c', 't1n', 't2f', 't2w']\n",
    "        image_data = []\n",
    "        for mod in modalities:\n",
    "            image_path = os.path.join(patient_path, f\"{patient_id}-{mod}.nii\")\n",
    "            image = nib.load(image_path).get_fdata()\n",
    "            image_data.append(image)\n",
    "\n",
    "        image_np = np.stack(image_data, axis=0).astype(np.float32)\n",
    "        image_tensor = torch.tensor(image_np, dtype=torch.float32)\n",
    "\n",
    "        # Load segmentation label if in training mode\n",
    "        if self.train:\n",
    "            label_path = os.path.join(patient_path, f\"{patient_id}-seg.nii\")\n",
    "            label_np = nib.load(label_path).get_fdata().astype(np.uint8)\n",
    "\n",
    "            # üîÑ Remap BraTS labels: [0, 1, 2, 4] ‚Üí [0, 1, 2, 3]\n",
    "            label_np[label_np == 4] = 3\n",
    "\n",
    "            # ‚úÖ FIX: Ensure label tensor is long\n",
    "            label_tensor = torch.from_numpy(label_np).long().unsqueeze(0)\n",
    "        else:\n",
    "            label_tensor = None\n",
    "\n",
    "        # Apply TorchIO preprocessing\n",
    "        if self.transform:\n",
    "            subject_dict = {\"images\": tio.ScalarImage(tensor=image_tensor)}\n",
    "            if label_tensor is not None:\n",
    "                subject_dict[\"label\"] = tio.LabelMap(tensor=label_tensor)\n",
    "            subject = tio.Subject(**subject_dict)\n",
    "            transformed = self.transform(subject)\n",
    "            image_tensor = transformed.images.data\n",
    "            if label_tensor is not None:\n",
    "                label_tensor = transformed.label.data\n",
    "\n",
    "        # üõ° Sanity check\n",
    "        if self.train:\n",
    "            if not torch.is_tensor(image_tensor) or not torch.is_tensor(label_tensor):\n",
    "                raise RuntimeError(f\"[ERROR] {patient_id}: invalid tensor types\")\n",
    "\n",
    "        return (image_tensor, label_tensor) if self.train else image_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "739a3063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 3: TorchIO Preprocessing Transform\n",
    "\n",
    "transform = tio.Compose([\n",
    "    tio.RescaleIntensity(out_min_max=(0, 1)),  # Normalize intensities to [0, 1]\n",
    "    tio.Resize((128, 128, 128)),               # Resize to fixed shape\n",
    "    tio.ZNormalization()                       # Normalize mean=0, std=1\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6278e077",
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_transform = tio.Compose([\n",
    "    # Intensity-based\n",
    "    tio.RandomBiasField(p=0.3),\n",
    "    tio.RandomGamma(p=0.3),\n",
    "    tio.RandomNoise(p=0.2),\n",
    "    \n",
    "    # Spatial-based\n",
    "    tio.RandomAffine(\n",
    "        scales=(0.9, 1.1),\n",
    "        degrees=10,\n",
    "        translation=5,\n",
    "        center='image',\n",
    "        p=0.5\n",
    "    ),\n",
    "    tio.RandomElasticDeformation(p=0.2),\n",
    "    tio.RandomFlip(axes=('LR',), p=0.5),\n",
    "\n",
    "    # Preprocessing\n",
    "    tio.RescaleIntensity(out_min_max=(0, 1)),\n",
    "    tio.Resize((128, 128, 128)),\n",
    "    tio.ZNormalization()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9aff78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÅ Loading Hospital1...\n",
      "üîÅ Loading Hospital2...\n",
      "üîÅ Loading Hospital3...\n",
      "üîÅ Loading Hospital4...\n",
      "üîÅ Loading Hospital5...\n",
      "üîÅ Loading Hospital6...\n",
      "üîÅ Loading Hospital7...\n",
      "üîÅ Loading Hospital8...\n",
      "üîÅ Loading Hospital9...\n",
      "\n",
      "‚úÖ All hospital loaders are ready.\n"
     ]
    }
   ],
   "source": [
    "# CELL 4: Client Dataset Paths and Loaders\n",
    "\n",
    "# Define paths (adjust as needed to match your directory structure)\n",
    "Hospital1_Train = 'Data/2023GLI/TrainingData'\n",
    "Hospital1_Val   = 'Data/2023GLI/ValidationData'\n",
    "Hospital2_Train = 'Data/2023MEN/TrainingData'\n",
    "Hospital2_Val   = 'Data/2023MEN/ValidationData'\n",
    "Hospital3_Train = 'Data/2023MET/TrainingData'\n",
    "Hospital3_Val   = 'Data/2023MET/ValidationData'\n",
    "Hospital4_Train = 'Data/2023PED/TrainingData'\n",
    "Hospital4_Val   = 'Data/2023PED/ValidationData'\n",
    "Hospital5_Train = 'Data/2023SSA/TrainingData'\n",
    "Hospital5_Val   = 'Data/2023SSA/ValidationData'\n",
    "Hospital6_Train_Val = 'Data/BraTS2021'\n",
    "Hospital7_Train = 'Data/BraTS2020/TrainingData'\n",
    "Hospital7_Val   = 'Data/BraTS2020/ValidationData'\n",
    "Hospital8_Train_Val = 'Data/BraTS2019/HGG'\n",
    "Hospital9_Train_Val = 'Data/BraTS2019/LGG'\n",
    "\n",
    "hospitals = {\n",
    "    \"Hospital1\": {\"train\": Hospital1_Train, \"val\": Hospital1_Val},\n",
    "    \"Hospital2\": {\"train\": Hospital2_Train, \"val\": Hospital2_Val},\n",
    "    \"Hospital3\": {\"train\": Hospital3_Train, \"val\": Hospital3_Val},\n",
    "    \"Hospital4\": {\"train\": Hospital4_Train, \"val\": Hospital4_Val},\n",
    "    \"Hospital5\": {\"train\": Hospital5_Train, \"val\": Hospital5_Val},\n",
    "    \"Hospital6\": {\"combined\": Hospital6_Train_Val},\n",
    "    \"Hospital7\": {\"train\": Hospital7_Train, \"val\": Hospital7_Val},\n",
    "    \"Hospital8\": {\"combined\": Hospital8_Train_Val},\n",
    "    \"Hospital9\": {\"combined\": Hospital9_Train_Val}\n",
    "}\n",
    "\n",
    "hospital_loaders = {}\n",
    "train_ratio = 0.8\n",
    "batch_size = 8\n",
    "\n",
    "for hospital, paths in hospitals.items():\n",
    "    print(f\"üîÅ Loading {hospital}...\")\n",
    "\n",
    "    if \"combined\" in paths:\n",
    "        #full_dataset = BraTSDataset(paths[\"combined\"], transform=transform, train=True)\n",
    "        full_dataset = BraTSDataset(paths[\"combined\"], transform=augment_transform, train=True)\n",
    "        train_size = int(train_ratio * len(full_dataset))\n",
    "        val_size = len(full_dataset) - train_size\n",
    "        train_set, val_set = random_split(full_dataset, [train_size, val_size])\n",
    "    else:\n",
    "        # train_set = BraTSDataset(paths[\"train\"], transform=transform, train=True)\n",
    "        # val_set = BraTSDataset(paths[\"val\"], transform=transform, train=False)\n",
    "\n",
    "        train_set = BraTSDataset(paths[\"train\"], transform=augment_transform, train=True)\n",
    "        val_set = BraTSDataset(paths[\"val\"], transform=transform, train=False)\n",
    "\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=8, shuffle=True,\n",
    "                          num_workers=2)\n",
    "    val_loader = DataLoader(val_set, batch_size=1, shuffle=False)\n",
    "\n",
    "    hospital_loaders[hospital] = {\n",
    "        \"train\": train_loader,\n",
    "        \"val\": val_loader\n",
    "    }\n",
    "\n",
    "print(\"\\n‚úÖ All hospital loaders are ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8093a3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_random_sample(hospital=\"Hospital1\", slice_idx=64):\n",
    "    loader = hospital_loaders[hospital][\"train\"]\n",
    "    images, labels = next(iter(loader))\n",
    "\n",
    "    # Shape: (B, C, D, H, W)\n",
    "    image = images[0]   # ‚Üí (4, D, H, W)\n",
    "    label = labels[0]   # ‚Üí (1, D, H, W)\n",
    "\n",
    "    print(f\"Image shape: {image.shape}, Label shape: {label.shape}\")\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    # Show FLAIR image (channel 3)\n",
    "    axs[0].imshow(image[3, :, :, slice_idx].cpu(), cmap='gray')\n",
    "    axs[0].set_title(f\"{hospital} - FLAIR slice {slice_idx}\")\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    # Show corresponding segmentation mask\n",
    "    axs[1].imshow(label[0, :, :, slice_idx].cpu(), cmap='Reds')\n",
    "    axs[1].set_title(f\"{hospital} - Mask slice {slice_idx}\")\n",
    "    axs[1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db6df417",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/awakili/anaconda3/envs/DLEnv/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/awakili/anaconda3/envs/DLEnv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n  File \"/home/awakili/anaconda3/envs/DLEnv/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 398, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/home/awakili/anaconda3/envs/DLEnv/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 211, in collate\n    return [\n  File \"/home/awakili/anaconda3/envs/DLEnv/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 212, in <listcomp>\n    collate(samples, collate_fn_map=collate_fn_map)\n  File \"/home/awakili/anaconda3/envs/DLEnv/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 155, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"/home/awakili/anaconda3/envs/DLEnv/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 272, in collate_tensor_fn\n    return torch.stack(batch, 0, out=out)\nRuntimeError: torch.cat(): input types can't be cast to the desired output type Long\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mvisualize_random_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHospital1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m, in \u001b[0;36mvisualize_random_sample\u001b[0;34m(hospital, slice_idx)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvisualize_random_sample\u001b[39m(hospital\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHospital1\u001b[39m\u001b[38;5;124m\"\u001b[39m, slice_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m):\n\u001b[1;32m      2\u001b[0m     loader \u001b[38;5;241m=\u001b[39m hospital_loaders[hospital][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Shape: (B, C, D, H, W)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     image \u001b[38;5;241m=\u001b[39m images[\u001b[38;5;241m0\u001b[39m]   \u001b[38;5;66;03m# ‚Üí (4, D, H, W)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/DLEnv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m~/anaconda3/envs/DLEnv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1480\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1478\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[1;32m   1479\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rcvd_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1480\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/DLEnv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1505\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1505\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/DLEnv/lib/python3.9/site-packages/torch/_utils.py:733\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/awakili/anaconda3/envs/DLEnv/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/awakili/anaconda3/envs/DLEnv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n  File \"/home/awakili/anaconda3/envs/DLEnv/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 398, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/home/awakili/anaconda3/envs/DLEnv/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 211, in collate\n    return [\n  File \"/home/awakili/anaconda3/envs/DLEnv/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 212, in <listcomp>\n    collate(samples, collate_fn_map=collate_fn_map)\n  File \"/home/awakili/anaconda3/envs/DLEnv/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 155, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"/home/awakili/anaconda3/envs/DLEnv/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 272, in collate_tensor_fn\n    return torch.stack(batch, 0, out=out)\nRuntimeError: torch.cat(): input types can't be cast to the desired output type Long\n"
     ]
    }
   ],
   "source": [
    "visualize_random_sample(\"Hospital1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9c9e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä Dataset sizes per hospital:\")\n",
    "for hospital, loaders in hospital_loaders.items():\n",
    "    train_size = len(loaders['train'].dataset)\n",
    "    val_size = len(loaders['val'].dataset)\n",
    "    print(f\"{hospital} ‚Üí Train: {train_size} | Val: {val_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6821c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 7: Fixed TwinSegNet Model (ViT + UNet Hybrid)\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_channels, skip_channels, out_channels):\n",
    "        super(UpBlock, self).__init__()\n",
    "        self.up = nn.ConvTranspose3d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "        self.conv = ConvBlock(out_channels + skip_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        x = self.up(x)\n",
    "        # Resize skip connection if needed\n",
    "        if x.shape[2:] != skip.shape[2:]:\n",
    "            skip = F.interpolate(skip, size=x.shape[2:], mode='trilinear', align_corners=False)\n",
    "        x = torch.cat((x, skip), dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class PatchEmbedViT(nn.Module):\n",
    "    def __init__(self, in_channels=128, embed_dim=256, patch_size=2):\n",
    "        super(PatchEmbedViT, self).__init__()\n",
    "        self.patch_embed = nn.Conv3d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, embed_dim, 1, 1, 1))  # minimal shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        if x.shape[2:] != self.pos_embed.shape[2:]:\n",
    "            pos_embed = F.interpolate(self.pos_embed, size=x.shape[2:], mode='trilinear', align_corners=False)\n",
    "        else:\n",
    "            pos_embed = self.pos_embed\n",
    "        return x + pos_embed\n",
    "\n",
    "\n",
    "class TwinSegNet(nn.Module):\n",
    "    def __init__(self, in_channels=4, n_classes=3, base_channels=32):\n",
    "        super(TwinSegNet, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.enc1 = ConvBlock(in_channels, base_channels)              # 128\n",
    "        self.pool1 = nn.MaxPool3d(2)\n",
    "        self.enc2 = ConvBlock(base_channels, base_channels * 2)        # 64\n",
    "        self.pool2 = nn.MaxPool3d(2)\n",
    "        self.enc3 = ConvBlock(base_channels * 2, base_channels * 4)    # 32\n",
    "        self.pool3 = nn.MaxPool3d(2)\n",
    "        self.enc4 = ConvBlock(base_channels * 4, base_channels * 8)    # 16\n",
    "        self.pool4 = nn.MaxPool3d(2)\n",
    "\n",
    "        # ViT at bottleneck\n",
    "        self.vit = PatchEmbedViT(in_channels=base_channels * 8, embed_dim=base_channels * 16, patch_size=2)\n",
    "        self.vit_proj = nn.Conv3d(base_channels * 16, base_channels * 8, kernel_size=1)\n",
    "\n",
    "        # Decoder (UNet-style)\n",
    "        self.up4 = UpBlock(base_channels * 8, base_channels * 4, base_channels * 4)  # match enc3\n",
    "        self.up3 = UpBlock(base_channels * 4, base_channels * 2, base_channels * 2)  # match enc2\n",
    "        self.up2 = UpBlock(base_channels * 2, base_channels, base_channels)          # match enc1\n",
    "        self.final_conv = nn.Conv3d(base_channels, n_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)                # [B, 32, 128,128,128]\n",
    "        e2 = self.enc2(self.pool1(e1))   # [B, 64, 64,64,64]\n",
    "        e3 = self.enc3(self.pool2(e2))   # [B, 128, 32,32,32]\n",
    "        e4 = self.enc4(self.pool3(e3))   # [B, 256, 16,16,16]\n",
    "        b = self.pool4(e4)               # [B, 256, 8,8,8]\n",
    "\n",
    "        b = self.vit(b)                  # [B, 512, 4,4,4] ‚Üí [B, 256, 4,4,4]\n",
    "        b = self.vit_proj(b)             # [B, 256, 4,4,4]\n",
    "\n",
    "        d4 = self.up4(b, e3)             # [B, 128, 8,8,8]\n",
    "        d3 = self.up3(d4, e2)            # [B, 64, 16,16,16]\n",
    "        d2 = self.up2(d3, e1)            # [B, 32, 32,32,32]\n",
    "        out = self.final_conv(d2)        # [B, n_classes, 32,32,32]\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94608a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(pred, target, epsilon=1e-6):\n",
    "    \"\"\"\n",
    "    pred: (B, C, D, H, W)\n",
    "    target: (B, 1, D', H', W') ‚Üí label mask\n",
    "    \"\"\"\n",
    "    pred = torch.softmax(pred, dim=1)\n",
    "\n",
    "    # Resize target to match pred spatial shape\n",
    "    if target.shape[2:] != pred.shape[2:]:\n",
    "        target = F.interpolate(target.float(), size=pred.shape[2:], mode='nearest')\n",
    "\n",
    "    # One-hot encode\n",
    "    target = target.squeeze(1).long()  # (B, D, H, W)\n",
    "    target = F.one_hot(target, num_classes=pred.shape[1])  # (B, D, H, W, C)\n",
    "    target = target.permute(0, 4, 1, 2, 3).float()          # (B, C, D, H, W)\n",
    "\n",
    "    # Dice computation\n",
    "    intersection = (pred * target).sum(dim=(2, 3, 4))\n",
    "    union = pred.sum(dim=(2, 3, 4)) + target.sum(dim=(2, 3, 4))\n",
    "    dice = (2 * intersection + epsilon) / (union + epsilon)\n",
    "\n",
    "    return 1 - dice.mean()\n",
    "\n",
    "\n",
    "def dice_coefficient(pred, target, epsilon=1e-6):\n",
    "    \"\"\"\n",
    "    Computes Dice coefficient for evaluation (not differentiable).\n",
    "    Assumes pred is logits and target is class index (not one-hot).\n",
    "    \"\"\"\n",
    "    pred = torch.argmax(torch.softmax(pred, dim=1), dim=1)\n",
    "    target = target.squeeze(1).long()\n",
    "\n",
    "    dice_scores = []\n",
    "    for class_id in range(1, pred.shape[1] if pred.ndim == 5 else 2):  # skip background\n",
    "        pred_class = (pred == class_id).float()\n",
    "        target_class = (target == class_id).float()\n",
    "\n",
    "        intersection = (pred_class * target_class).sum()\n",
    "        union = pred_class.sum() + target_class.sum()\n",
    "\n",
    "        dice = (2 * intersection + epsilon) / (union + epsilon)\n",
    "        dice_scores.append(dice.item())\n",
    "\n",
    "    return np.mean(dice_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edc197e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import gc\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "def train_one_client(model, dataloader, optimizer, epochs=1, client_name=\"\", device=\"cuda:0\"):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    scaler = GradScaler()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        print(f\"\\nüöÄ [{client_name}] Epoch {epoch+1}/{epochs} ‚Äî Training {len(dataloader)} batches\")\n",
    "\n",
    "        for i, (images, masks) in enumerate(dataloader):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast():\n",
    "                outputs = model(images)\n",
    "                loss = dice_loss(outputs, masks)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            print(f\"   üì¶ Batch {i+1}/{len(dataloader)} ‚Äî Loss: {loss.item():.4f} ‚Äî \"\n",
    "                  f\"Mem: {torch.cuda.memory_allocated(device) / 1e6:.1f} MB\")\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        print(f\"‚úÖ [{client_name}] Epoch {epoch+1} Complete ‚Äî Avg Loss: {epoch_loss / len(dataloader):.4f}\")\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    return model.state_dict()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3b5edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "def clear_cuda():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "    print(f\"üßπ GPU memory cleared. Available: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bc3981",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "!fuser -v /dev/nvidia0  # see PIDs using the GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fd4270",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill -9 <3013>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5087e91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b68de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_devices = {\n",
    "    \"Hospital1\": \"cuda:0\",\n",
    "    \"Hospital2\": \"cuda:1\",\n",
    "    # add more mappings as needed, alternating between 0 and 1\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8ac31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "client_name = \"Hospital1\"\n",
    "device = client_devices[client_name]\n",
    "model = TwinSegNet(in_channels=4, n_classes=4).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "train_loader = hospital_loaders[client_name][\"train\"]\n",
    "\n",
    "new_weights = train_one_client(model, train_loader, optimizer, epochs=2, client_name=client_name, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4403061",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1fb1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504fe853",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory_summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e0265e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
